---
title: "Lecture 5"
subtitle: "The Effects Model"
author: "Psych 10 C"
institute: "University of California, Irvine"
date: "04/08/2022"
output:
  xaringan::moon_reader:
    self_contained: true
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-extra, echo = FALSE}
xaringanExtra::use_tile_view()

xaringanExtra::use_fit_screen()

xaringanExtra::use_editable(expires = 1)

xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)

htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```

```{r load-tidy, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)

library(flipbookr)

knitr::opts_chunk$set(fig.width = 6, message = FALSE, 
                      warning = FALSE, comment = "", 
                      cache = F)
link <- "https://raw.githubusercontent.com/ManuelVU/psych-10c-data/main/example-smoke.csv"
smokers <- read_csv(file = link)
```

## Null Model

- Last class we used mathematical notation and the Normal distribution to 
represent what we called the **Null Model**

--

- The Null model assumed that observations from every participant $i=1,\dots,I$ 
are sampled from the same distribution, regardless of their group $j=1,2$.

--

- The Null model formalized an initial verbal hypothesis: that there are **no 
differences between groups**.

--

- The **Null Model** is defined as: $$y_{ij} \sim \text{Normal}(\mu, \sigma^2)$$

--

- Finally, we said that since we don't know the values of the parameters $\mu$ 
and $\sigma^2$ that fully define the Normal distribution, we needed to infer 
(learn) them from our observations.

--

- This can be done through a process called **Statistical Inference**.

---

## Statistical Inference for the Normal distribution

- One of the key advantages of the Normal distribution is that its parameters 
can be directly tied back to the Expectation and Variance of the random variable.

--

- In other words, the Null model assumes that our observations are random 
variables that follow a Normal distribution with expected value $\mu$ and 
and variance $\sigma^2$.

--

- If a random variable $y$ follows a Normal distribution with parameters $\mu$ 
and $\sigma^2$, then we know that the following two statements are **TRUE**:
$$\mathbb{E}(y) = \mu$$ and $$\mathbb{V}ar(y) = \sigma^2$$

--

- Remember that we already have a very good approximation for both $\mathbb{E}(y)$
and to the $\mathbb{V}ar(y)$!

---

class: inverse, middle, center

# Estimators

---

## Estimators

- In Week 1 we talked about statistics as functions of our observations.

--

- When a statistic is used to approximate a parameter (e.g. $\mu$) in a 
statistical model, it is called an **estimator**.

--

- Since we know that our best statistic for the expected value of a r.v. is
the mean (i.e. the average of our observations), we can use it as an 
**estimator** for $\mu$.

--

- We represent these estimators by adding a "hat" on top of the Greek character
corresponding to the parameter they estimate: 
$$\hat{\mu} = \frac{1}{n} \sum_{j} \sum_{i} y_{ij}$$

--

- Here, $n$ represents the total number of observations that we add together to 
calculate the mean, while the indices $i$ and $j$ denote the observation number 
and the group, respectively.

---

## Variance estimator

- We also said that a good approximation for the variance of a r.v. is the 
sample variance $s^2$. However, this time we will write it slightly different
to make other models easier to understand.

--

- Our estimator for the variance will be denoted as: 
$$\hat{\sigma}^2_0 = \frac{1}{n} \sum_j \sum_i \left(y_{ij}-\hat{\mu}\right)^2$$

--

- If you look at your previous notes, you will see that we replaced the sample 
mean $\bar{y}$ with our **estimator** $\hat{\mu}$, and that now we refer to it 
as $\hat{\sigma}_0^2$ rather than $s^2$.

--

- The subscript "0" indicates that this estimator corresponds to the variance of 
the **Null Model**. 

---

## The Null Model

- In the case of our Null model, we can use our new estimators $\hat{\mu}$ 
and $\hat{\sigma}_0^2$ to refer to:

--

  1. The Model Prediction: $\hat{\mu}$
  
--

  1. The Mean Squared Error: $\hat{\sigma}_0^2$
  
--

- The variance estimator $\hat{\sigma}_0^2$ is referred to as the Mean Squared 
Error because it captures the average distance (in squared units) between what 
we expect to observe ( $\hat{\mu}$ ) and what we actually observed ( $y_{ij}$ ).

--

- When using this type of statistical models (as we'll do for most of this 
class), we will also be interested on the Sum of Squared Errors, which is 
denoted as: 
$$SSE_0 = \sum_j \sum_i \left(y_{ij}-\hat{\mu}\right)^2$$

--

- Again, we add "0" as a subscript to make it clear that this Sum of Squared 
Errors is associated to the **Null Model**.

---

- Form teams of 3 and calculate the model prediction, the sum of squared errors 
and the mean squared error that would correspond to the **Null Model** for the 
smokers data:

```{r show-data, echo = FALSE}
DT::datatable(data = smokers,
  fillContainer = FALSE, options = list(pageLength = 8))
```

---

## Smokers data: Null Model

.can-edit.key-likes[
- Prediction: 
]

.can-edit.key-likes[
- SSE: 
]

.can-edit.key-likes[
- Mean Squared Error: 
]

---

class: inverse, middle, center

# The Effects Model

---

## Effects Model

- Now we will formalize our second hypothesis.

--

- Our original problem stated that we wanted to know if there were any 
differences in lung capacity between smokers and non-smokers.

--

- Our first model was constructed from the idea that there are no differences 
between groups.

--

- We will call the model that assumes that **groups are different** the 
**Effects Model**.

---

## Effects Model

- Following the logic we used to formalize the Null Model, we will keep on using
our observations' notation $y_{ij}$ and the Normal distribution as the basis for
this second statistical model.

--

- In mathematical notation, the **Effects Model** would be: 
$$y_{i1}\sim\text{Normal}(\mu_1,\sigma_e^2)$$ $$y_{i2}\sim\text{Normal}(\mu_2,\sigma_e^2)$$

--

or $$y_{ij}\sim\text{Normal}(\mu_j,\sigma_e^2)$$

--

- Both ways of writing the model convey the same information, but the second 
one is just shorter.

--

- This new statistical model formalizes the idea that the $i = 1, \dots, 4$ 
observations on each group $j = 1, 2$ are sampled from two different Normal 
distributions, one centered at $\mu_1$ and the other one at $\mu_2$.

---

## Effects Model

- Another important thing to notice from our definition of the model is that we 
assume that both distributions have the same variance $\sigma_e^2$.

--

- This means that our model assumes that the only difference between the 
two groups is their expected value and not how much they vary.

--

- We can now use all of our observations to calculate a single error estimate 
for this model.

--

- Why is it called the **Effects Model**?

--

- The difference between $\mu_1$ and $\mu_2$ is interpreted as the group-effect. 
In other words, the difference on our random variable that comes from being a 
member of either group 1 or group 2.

--

- In our smoking example, the difference between $\mu_2$ and $\mu_1$ would 
capture the effect that smoking has on lung capacity.

--

- How much do we *expect* lung capacity to differ between a person that smokes 
and a person that does not smoke.

---

# Graphical Representation Effects Model

- As we did with the Null Model, we can also represents the Effects model 
graphically.

--

- Notice that once again, we don't have values for our parameters, but we can 
still make a graph that shows that we expect each group to be represented by a 
different Normal distribution, which differ in their mean but share the same 
variance.

--

- We can use the following R code to generate this visualization:

.pull-left[
```{r normal-effect, eval = FALSE}
par(mai = c(1,0.1,0.1,0.1))
curve(dnorm(x, mean = 0, sd = 1), from = -4, to = 6,
      axes = FALSE, ann = FALSE, col = "red", lwd = 3)
curve(dnorm(x, mean = 2, sd = 1), col = "blue", add = T, lty = 3, lwd = 3)
box(bty = "l")
mtext(text = "Lung capacity", side = 1, line = 2, cex = 1.6)
legend("topleft", bty = "n", col = c("blue","red"), 
       legend = c("non-smokers", "smokers"), lty = c(3, 1))
```
]

.pull-right[
```{r normal-effect-out, ref.label = "normal-effect", fig.align = 'center', echo = FALSE, fig.height = 4.6}

```
]

---

## Graphical Representation

- Notice that in the previous plot we did not assign any specific values to the 
variable lung capacity.

--

- This is because at this point we only want to convey the fact that the Effects
model assumes that observations collected on each group are sampled from their 
own Normal distribution.

--

- We can (and should) formalize our theories about the world before even 
looking at the data.

--

- In other words, we will always specify our models before even running the 
experiment (so we can't estimate the values of our parameters).

---

## Estimators for the Effects model

- Now that we have specified our new model (that assumes that the groups are 
different), we have 3 parameters for which we want to find appropriate 
estimators.

--

- Our parameters are $\mu_1$, $\mu_2$ and $\sigma_e^2$. Again, our estimators 
will be a statistic (a function of the sample) and we will denote them with a 
hat.

--

$$\hat{\mu}_1 = \frac{1}{n_1} \sum_i y_{i1}$$ 

--

$$\hat{\mu}_2 = \frac{1}{n_2} \sum_i y_{i2}$$

--

and finally, 

$$\hat{\sigma}_e^2 = \frac{1}{n} \sum_j \sum_i (y_{ij} - \hat{\mu}_j)^2$$

- Where $n$ represents the total number of observations, and $n_j$ represents 
the number of observations in group $j = 1, 2$

---

## Estimators for the Effects Model

- Notice that our estimators for $\mu_1$ and $\mu_2$ are just the mean of the
observations collected by group!

--

- Again, we can interpret $\hat{\mu}_j$ as the prediction of the model for each
observation $i = 1, \dots, 4$ of group $j = 1, 2$. 

--

- Similarly, we can interpret our estimator for $\hat{\sigma}_e^2$ as the 
average error in the predictions of the model. In other words, how far on 
average are our observations from the prediction $\hat{\mu}_j$ made by group.

--

- As with the Null model, we are now interested in calculating the total error
of the model: the Sum of Squared Error of the **Effects Model**.

--

- We will denote the Sum of Squared Error of the **Effects Model** as:
$$SSE_e = \sum_j \sum_i (y_{ij} - \hat{\mu}_j)^2$$

---

- Form teams of 3 and calculate the model predictions for each group, the sum of 
squared errors and the mean squared error of the **Effects Model** for the 
smokers data:

```{r show-data2, echo = FALSE}
DT::datatable(data = smokers,
  fillContainer = FALSE, options = list(pageLength = 8))
```

---

## Smokers data: Effects Model

.can-edit.key-likes[
- Prediction:
  - non-smokers =
  - smokers =
]

.can-edit.key-likes[
- SSE: 
]

.can-edit.key-likes[
- Mean Squared Error: 
]


