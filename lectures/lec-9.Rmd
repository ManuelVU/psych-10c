---
title: "Lecture 9"
subtitle: "Multiple Groups: Additional models"
author: "Psych 10 C"
institute: "University of California, Irvine"
date: "04/15/2022"
output:
  xaringan::moon_reader:
    self_contained: true
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-extra, echo = FALSE}
xaringanExtra::use_tile_view()

xaringanExtra::use_fit_screen()

xaringanExtra::use_editable(expires = 1)

xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)

htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```

```{r load-tidy, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)

knitr::opts_chunk$set(fig.width = 6, message = FALSE, 
                      warning = FALSE, comment = "", 
                      cache = F)  
link <- "https://raw.githubusercontent.com/ManuelVU/psych-10c-data/main/anxiety-2-ex.csv"
anxiety <- read_csv(file = link)
```

## Final Project

- As you already know, we will have a final project for this class instead of a 
final exam.

--

- This final project is worth 50% of your total grade and it should be done in 
groups of between 3 and 4 students.

--

- The objective is to have you write a short version of what a scientific paper 
should look like. Including the data analysis section.

--

- To make things "easier" for you, we have selected 3 problems with their 
corresponding data sets that you can choose from for your final project.

--

- To answer the research question of each project you will need a different 
approach, not all problems can be solved the same way. Therefore, before we 
look at the data we want you to choose one of the three problems.

---

### Problem 1:

- We want to know if people that have had a brain curgical procedure known as 
split brain, have problems identifing obhects presented at either side of their 
visual field.

--

### Problem 2:

- We want to know if the cognitive decline of a person from one year to the next 
is associated with their level of social interaction, physical activity and the 
fact that they used the app Luminosity during that year.

--

### Problem 3:

- We want to know how the asking price for an IKEA chair changes as a function
of the total cost of the matrials, if the person build the chaitr themselves and
the difficulty of building it.

---

## Final project

The paper must have the following points:

- Introduction: 4 to 5 sentences that summarize the problem, and why it's 
important. The idea is to convince your reader that what you are doing matters
you can use references to previous work (1 or 2 max).

--
  
- Methods: In one paragraph, explain what is in the data set and where it came 
from (how it was collected).

--
  
- Data: provide a summary of your data, you can use summary statistics like
the median, mean or variance. You can also use this section to highlight some
properties of the data that you are working on by using graphs.
  
---

## Final project

- Model comparison: Describe the models that you will be comparing using your 
current data and their main assumptions (does the model assume that groups are
equal?).
  
  - You should provide a summary of your analysis, a good way to do so can be
  to have a table that has the SSE of each model that you tested and the BIC
  associated to this model.

--

- Discussion: 

  - One paragraph on: What conclusions can you draw from the model comparisons
  about the experimental question.
  
--
  
  - One paragraph on: What might be the broader implications of this finding?
  
--
  
  - One paragraph on: What are the limitations of this study and/or what are the 
  gaps left by this study? Be sure to explain why the limitation could impact 
  the results.
  
---

class: inverse, center, middle

# Review of BIC's

---

## BIC

- Last week we mentioned that regardless of what data we have, a model that 
assumes that there is more that one distribution will always be better in 
terms of the error it produces in comparison to a simpler model.

--

- We can try to understand this intuitively by looking a graph that uses 
a single mean as a prediction. To do this we will use our smokers data:

--

```{r, echo = FALSE, fig.align='center', fig.height = 6, fig.width = 7}
link <- "https://raw.githubusercontent.com/ManuelVU/psych-10c-data/main/example-smoke.csv"
smokers <- read_csv(file = link)
par(mai = c(0.6,0.6,0.6,0.6))
par(oma = c(3, 3, 0.2, 0.2))
plot(x = seq(1,nrow(smokers)), y = sort(smokers$lung_capacity, decreasing = FALSE),
     axes = FALSE, ann = FALSE, col = rep(c("#c80064","#54bebe"), each = 4),
     pch = rep(c(15,19), each = 4), cex = 1.6)
abline(h = mean(smokers$lung_capacity), lwd = 3, col = "#003f5c")
box(bty = "l", lwd = 1.3)
axis(2, las = 2)
mtext(text = "Lung capacity", line = 2.3, cex = 1.6, side = 2)
mtext(text = "Smokers", line = 1, cex = 1.8, side = 1, at = 2.5)
mtext(text = "Non-smokers", line = 1, cex = 1.8, side = 1, at = 6.5)
```


---

## BIC

- If we use two predictions instead of one (as in the effects model) the error
will be reduced.

--

```{r, echo = FALSE, fig.align='center', fig.height = 6, fig.width = 7}
link <- "https://raw.githubusercontent.com/ManuelVU/psych-10c-data/main/example-smoke.csv"
smokers <- read_csv(file = link)
par(mai = c(0.6,0.6,0.6,0.6))
par(oma = c(3, 3, 0.2, 0.2))
plot(x = seq(1,nrow(smokers)), y = sort(smokers$lung_capacity, decreasing = FALSE),
     axes = FALSE, ann = FALSE, col = rep(c("#c80064","#54bebe"), each = 4),
     pch = rep(c(15,19), each = 4), cex = 1.6)
abline(h = mean(smokers$lung_capacity), lwd = 1.5, col = "#003f5c", lty = 2)
segments(x0 = 0,y0 = mean(smokers$lung_capacity[smokers$smoke_status=="smoker"]),
         x1 = 4.5, y1 = mean(smokers$lung_capacity[smokers$smoke_status=="smoker"]),
         col = "#c80064", lwd = 3)
segments(x0 = 4.5,y0 = mean(smokers$lung_capacity[smokers$smoke_status=="non_smoker"]),
         x1 = 9, y1 = mean(smokers$lung_capacity[smokers$smoke_status=="non_smoker"]),
         col = "#54bebe", lwd = 3)
box(bty = "l", lwd = 1.3)
axis(2, las = 2)
mtext(text = "Lung capacity", line = 2.3, cex = 1.6, side = 2)
mtext(text = "Smokers", line = 1, cex = 1.8, side = 1, at = 2.5)
mtext(text = "Non-smokers", line = 1, cex = 1.8, side = 1, at = 6.5)
```

---

## BIC

- In general, we know that the more parameters (lines in the previous example)
our models have, the more they will resemble the values in our data (less 
error).

--

- This is known as "fit", and for linear models adding more parameters or 
unknowns (like adding more values of $\mu$) to our models will always improve 
their fit (how close our predictions are to our observations).

--

- If we only use "fit" or error as a criterion of which theory is better, then 
we could always add more an more lines to the graph until we get a single line 
for each point.

--

- However, this is not be useful as it tells us that in order to make a 
prediction we need to know the value that we want to predict. This doesn't make
sense.

---

## BIC

- To avoid this problem we would like to take into account two things:

--

1. How many parameters (unknown values) we use to make predictions (model
complexity).

--

2. How close those predictions are to those values (model fit).

--

- BIC is one of the methods (there are more) that can accomplish this objective, 
and it does so by assigning a value to how bad the predictions of the model are 
(how large is the total error) and how many parameters (unknown values) the 
model uses to make it's predictions.

--

- The BIC will assign higher values to models that make larger errors or that 
need more parameters (unknown values) to make a prediction.

--

- If we select the model with the lowest BIC from the ones that we are using 
then we are making a decision that weights both things, the complexity of the 
model and the accuracy of the model (fit).

---

class: inverse, center, middle

# Multiple Groups

---

## Comparison between multiple groups

- Last class we started looking at an example where we had the level of anxiety 
of 3 cohorts of students at a university. The 2018, 2019 and 2020 cohorts.

--

- We formalized a model that assumed that there are no difference on the levels 
of anxiety of First year students on the 3 cohorts (Null model) that states: $$y_{ij}\sim\text{Normal}(\mu,\sigma_0^2)$$
for $i=1, \dots,30$ students in each of the $j=1,2,3$ cohorts (2018 to 2020).

--

- The second model assumes that the levels of anxiety of First year students in
one of the three cohorts is different from the rest. Formally, this model is 
expressed as: $$y_{ij}\sim\text{Normal}(\mu_j,\sigma_e^2)$$ 

--

- We also mentioned that this model does not address our original question, as 
the conclusion that we can draw from it is that there is at least one of the 
groups is different from the rest.

---

## Results: review

- The estimations for the Null model are:

.pull-left[
```{r null-groups}
n_total <- nrow(anxiety)
anxiety <- anxiety %>% 
  mutate("null_pred" = mean(anxiety),
         "null_error" = (anxiety - null_pred)^2)
sse_0 <- sum(anxiety$null_error)
mse_0 <- 1/n_total * sse_0
```
]
.pull-right[
Prediction: `r round(mean(anxiety$anxiety), 2)`

SSE: `r round(sse_0, 2)`

Mean SE: `r round(mse_0, 2)`
]

---

## Results: review

- The estimations from the Effects model are:

.pull-left[
```{r eff-groups}
mean_groups <- anxiety %>% 
  group_by(cohort) %>% 
  summarise("pred" = mean(anxiety))
anxiety <- anxiety %>% 
  mutate("eff_pred" = case_when(cohort == "2018" ~ mean_groups$pred[1],
                                cohort == "2019" ~ mean_groups$pred[2],
                                cohort == "2020" ~ mean_groups$pred[3]),
         "eff_error" = (anxiety - eff_pred)^2)
sse_e <- sum(anxiety$eff_error)
mse_e <- 1/n_total * sse_e
```
]
.pull-right[
Prediction: 
  - 2018: `r round(mean_groups$pred[1], 2)`
  - 2019: `r round(mean_groups$pred[2], 2)`
  - 2020: `r round(mean_groups$pred[3], 2)`
  
SSE: `r round(sse_e, 2)`

Mean SE: `r round(mse_e, 2)`
]

---

## Results: review

- The conclusion that we draw from those estimations was that:

--

  1. The effects model accounted for `r 100*round((sse_0-sse_e) / sse_0, 3)`% 
  of the total variation in anxiety scores of the students on the 3 different 
  cohorts.
  
--
  
  2. According to the BIC values, we should select the Effects model 
  (`r round(n_total * log(mse_e) + 3 * log(n_total),2)`) over the Null 
  (`r round(n_total * log(mse_0) + log(n_total),2)`). This means, that at least 
  one of the 3 groups is different from the others.
  
--

- However, our original problem stated that we wanted to know if there where any
differences between the cohorts, and not that there was at least one cohort that
was different.

--

- To solve this problem we have to look at three additional models.

---

## Group models

- From the results of the comparison between the Null and Effects model we know 
that there is at least one cohort of students that has a different anxiety level
in comparison to the others.

--

- In order to find out which group is different, we can build three new 
models that assume that a single cohort is different for the rest.

--

- Notice that making the comparison between Null and Effects model at the start
will save us a lot of work if it turns out that the best model is the Null. 

---

## Three models

- We need 3 models that formalize 3 assumptions: 

--
1. Students in the 2018 cohort are different from students in the 2019 and 2020 
cohorts.

--

2. Students in the 2019 cohort are different from students in the 2018 and 2020 cohorts.

--

3. Students in the 2020 cohort are different from students in the 2018 and 2019 
cohorts.

--

- To make this easy to remember we will refer to each model using the year 
associated with the cohort that is **different** from the rest. (e.g the "2018 
model" is the one that assumes that students in the 2018 cohort are different 
from students in the other 2).

---

## 2018 model

- We can formalize our "2018 model" using the following notation:
$$y_{i1} \sim \text{Normal}(\mu_1,\sigma_1^2)$$ $$y_{i2},y_{i3} \sim \text{Normal}(\mu,\sigma_1^2)$$
--

- Notice that only the expected value of the anxiety level of students in the 
2018 cohort $(\mu_1)$ is different from the other two $(\mu)$. 

--

- Our estimators for $\mu_1$, $\mu$ and $\sigma_1^2$ will be similar to the ones
we had before.

--

- First the estimator $\hat{\mu}_1$ will be the average anxiety level in the 2018 
cohort, while the estimator for the other 2 cohorts $\hat{\mu}$ will be the 
average anxiety level of all students on the 2019 and 2020 cohorts.

---

## 2018 model: Predictions

- An easy way to calculate and add the predictions of this new model to our data 
is by first creating an "indicator" variable. This is a variable that takes one
value if the observation belongs to the 2018 cohort and another value if it 
belongs to one of the others.

```{r id-var}
anxiety <- anxiety %>% 
  mutate("id_2018" = ifelse(test = cohort == "2018", yes = "2018", 
                            no = "other"))
```

--

- Then we can use that variable to group our data and get the averages that we
need.

--

```{r 2018-pred}
pred_2018 <- anxiety %>% 
  group_by(id_2018) %>% 
  summarise("pred" = mean(anxiety))
```

--

- The predicted anxiety level for First year students in the 2018 cohort was
`r round(pred_2018$pred[1],2)` and for students in the 2019 and 2020 cohorts it was 
`r round(pred_2018$pred[2],2)`.

---

## 2018 model: Error

- Using the predictions of the anxiety levels we can calculate the error of the 
model. Again, we start by calculating the error for each observation by taking the difference between the observation and the model prediction ( $\hat{\mu}_1$ and $\hat{\mu}$, respectively) and then squaring that difference.

```{r error}
anxiety <- anxiety %>% 
  mutate("prediction_2018" = ifelse(test = id_2018 == "2018", 
                                    yes = pred_2018$pred[1],
                                    no = pred_2018$pred[2]),
         "error_2018" = (anxiety - prediction_2018)^2)
```

--

- Once again we can get the SSE for the model by adding all of the values in 
the `error_2018` column. We can also use those values to calculate the 
proportion of variance accounted for by the model that assumes that only 
students from the 2018 cohort have different aciety levels.





