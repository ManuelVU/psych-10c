---
title: "Lecture 18"
subtitle: "Simple Linear Regression"
author: "Psych 10 C"
institute: "University of California, Irvine"
date: "05/11/2022"
output:
  xaringan::moon_reader:
    self_contained: true
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-extra, echo = FALSE}
xaringanExtra::use_tile_view()

xaringanExtra::use_fit_screen()

xaringanExtra::use_editable(expires = 1)

xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)

htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```

```{r load-tidy, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(flipbookr)

knitr::opts_chunk$set(fig.width = 6, message = FALSE, 
                      warning = FALSE, comment = "", 
                      cache = F)
grades <- read_csv(file = here::here("data/week-7/sim-examples.csv"))
```

## Simple linear regression

- Last class we mentioned that we can use a line in order to make predictions
about the values of a continuous dependent variable when we have information
about an independent variable.

--

- We also mentioned that the equation of the line is defined by two parameters: 

--

  - $\beta_0$ which is known as the **intercept** and it can be interpreted as 
  the expected value of the dependent variable when the independent variable
  is equal to 0.
  
--

  - $\beta_1$ which is known as the **slope** which is the change in the 
  expectation of our dependent variable for a **unit** increase in the value
  of our independent variable.
  
--

- The problem was that each combination of values of $\beta_0$ (**intercept**) 
and $\beta_1$ (**slope**) will give us a different line, so we need a way to
choose the best one.

---

## Least Squares

- The method that we use to find the values of $\beta_0$ and $\beta_1$ is known
as Least Squares. The idea is that we want to choose the values of the 
parameters that minimize the error of the predictions in the model.

--

- In other words, we want to find the values that minimize the Sum of Squared 
Errors (SSE).

.pull-left[
```{r miss-grade-lm1, echo=FALSE, fig.align='center',fig.width=7, fig.height=6}
par(oma = c(0,0,0,0),
    mai = c(1,1,0,0))
x <- c(2,4,6,8)
y <- 98 - 5.2 * x + c(7,-13,11,-5)
sse1 <- sum((y - (82 - 3.6 * seq(2,8,2)))^2)
plot(x, y, axes = F, ann = F,
      cex = 2, ylim=c(50,100), type = "n")
arrows(x0 = seq(2,8,2),y0 = 82 - 3.6 * seq(2,8,2), x1 = seq(2,8,2),
      y1 = y, length = 0.2, lty = 2, lwd = 3)
points(x, y, col = "#0D95D0", pch = 16, cex = 2.5)
box(bty = "l")
axis(side = 1, cex.axis = 1.8)
axis(side = 2, las = 2, cex.axis = 1.8)
mtext(text = "Missed classes", side = 1, line = 3, cex = 2.5)
mtext(text = "Grade", side = 2, line = 3, cex = 2.5)
curve(82 - 3.6* x, from = 2, to = 8, lwd = 2.5, col = "#774fa0", add = TRUE)
legend("topright", legend = c(expression(paste(beta[0], " = 82")),
                              expression(paste(beta[1], " = -3.6")),
                              expression(paste(SSE, " = 709"))),
       bty = "n", cex = 2.8)
```
]

.pull-right[
```{r miss-grade-lm2, echo=FALSE, fig.align='center',fig.width=7, fig.height=6}
par(oma = c(0,0,0,0),
    mai = c(1,1,0,0))
sse2 <- sum((y - (105 - 6 * seq(2,8,2)))^2)
plot(x, y, axes = F, ann = F, ylim=c(50,100), type = "n")
arrows(x0 = seq(2,8,2),y0 = 105 - 6 * seq(2,8,2), x1 = seq(2,8,2),
      y1 = y, length = 0.2, lty = 2, lwd = 3)
points(x, y, col = "#0D95D0", pch = 16, cex = 2.5)
box(bty = "l")
axis(side = 1, cex.axis = 1.8)
axis(side = 2, las = 2, cex.axis = 1.8)
mtext(text = "Missed classes", side = 1, line = 3, cex = 2.5)
mtext(text = "Grade", side = 2, line = 3, cex = 2.5)
curve(105 - 6 * x, from = 2, to = 8, lwd = 2.5, col = "#774fa0", add = TRUE)
legend("topright", legend = c(expression(paste(beta[0], " = 105")),
                              expression(paste(beta[1], " = -6")),
                              expression(paste(SSE, " = 393"))),
       bty = "n", cex = 2.8)
```
]

---

## Simple linear regression

- Before we find the values of the parameters for the simple linear regression
model, we want to formalize the models that we are going to compare.

--

- Given that we have a single independent variable $x_i$, we will only need to 
compare two models, one that assumes that the independent variable has no 
effect on the values of the dependent variable, and the other one that assumes 
that the independent variable has an effect.

--

- Comparing those two models will let us decide if our independent variable is a 
**good predictor** of the dependent variable.

--

- Before we introduce the models that we want to compare in a simple linear 
regression let's expand the example with grades and missed classes.

---

### Example: grades and classes missed

- We want to know how the grade that a student gets (dependent variable) changes 
as a function of the number of classes they missed during the quarter 
(independent variable).

--

- In other words, we want to know if the number of classes missed by a student
is a good predictor of their final grade.

--

- We have the final grade and number of classes missed by 130 students in a 
statistics class:

--

```{r print-data, echo = FALSE}
DT::datatable(data = grades,
  fillContainer = FALSE, options = list(pageLength = 4))
```


---

## Models for simple linear regression

- The first model that we need to compare is the Null model.

--

- The Null model in a simple linear regression formalizes the assumption that 
the expected value of our dependent variable is constant regardless of the 
values of the predictor (independent variable). 

--

- This model is expressed formally as: $$y_i \sim \text{Normal}(\beta_0, \sigma_0^2)$$

--

- Were $i$ denotes the observation number.

--

- In our example, this model assumes that the expected grade of a student is 
independent of the number of classes they missed.

--

- Notice that the only difference between this Null model and the ones that we have 
seen before is that we denote the expectation as $\beta_0$ instead of $\mu$.

--

- Similarly, our best guess for the value of $\beta_0$ for the Null model will 
be the group average.

---

## Simple linear regression model

- The second model that we need to evaluate is the linear regression model.

--

- This model assumes that the expected value of our dependent variable is a 
linear function of the number of classes missed by a student.

--

- Formally, the model is expressed as: 
$$y_i \sim \text{Normal}(\ \beta_0+\beta_1x_i,\sigma_1^2)$$

--

- In other words, the model assumes that all observations that share the same 
value of the predictor $x_i$ follow the same distribution. In other words, this 
distribution is not the same for every value of $x$.

--

- In our grades example this means that the model assumes that the expected
grade of students that missed 3 classes is $\beta_0 +\beta_1\ (3)$ and that
distribution is different from students that missed 5, 6 or any other number of
classes.

--

- Notice that the variance is the same regardless of the number of classes that 
the student missed.

---

## Simple linear regression

- Another way to think about this is that the linear model predicts a different
expected grade for each number of classes missed.

--

- This would be similar to the multiple groups case that we talked about on week 
4.

--

- However, a simple linear regression has the advantage that it assumes that 
the change in the expected value of the dependent variable should follow a 
straight line.

--

- Now we can look at the values of the parameters for each model.

---

## Predictions: Null model

- As we said before, the Null model assumes that the expected grade of a student 
is constant (doesn't change) as a function of the number of classes they missed.

--

- Given that the model has only one parameter, our best guess for the value of 
that parameter will be the average of the dependent variable of all participants. 
This is the same estimator that we have used for the Null model before.

--

```{r null-pred}
n_total <- nrow(grades)

null <- grades %>% 
  summarise("pred" = mean(grade)) %>% 
  pull(pred)

grades <- grades %>% 
  mutate("prediction_null" = null,
         "error_null" = (grade - prediction_null)^2)

sse_null <- sum(grades$error_null)
mse_null <- 1/n_total * sse_null
bic_null <- n_total * log(mse_null) + 1 * log(n_total)
```

---

## Simple linear regression

- Now we need to get the values for $\hat{\beta}_0$ and $\hat{\beta}_1$ for the 
simple linear regression.

--

- There is no function that I know in `tidyverse` that will allow us to do this,
however, it is easy to do in base R.

--

- The R function to do a linear regression in base R is `lm()`.

--

- There are 2 important arguments of this function that we need to use. The 
first one is `formula =` and it requires us to express the model as:
$$\text{dependent_variable} \sim \text{independent_variable}$$ 

--

- The second argument for the function is the `data =` where we have to indicate
the name of the object that contains our observations.

---

## Parameters: Simple linear regression

- First we will get the values of $\hat{\beta}_0$ and $\hat{\beta}_1$.

```{r lm-betas}
betas <- lm(formula = grade ~ classes_missed, data = grades)$coef
```

--

- The function `lm()` will return multiple values so to get only the values of 
$\hat{\beta}_0$ and $\hat{\beta}_1$ we need to use `$coef` after the function.

--

- Now we can add the predictions and errors of the linear model to our data, 
remember that the prediction of the linear model will be:
$$\hat{\beta}_0 + \hat{\beta}_1 \text{classes_missed}_i$$

--

```{r lm-pred}
grades <- grades %>% 
  mutate("prediction_linear" = betas[1] + betas[2] * classes_missed,
         "error_linear" = (grade - prediction_linear)^2)
```

---

## Model comparison

- As with the previous problems, we can get the SSE, Mean SE, $R^2$ and BIC for
the models in a linear regression.

--

```{r lm-compare}
sse_linear <- sum(grades$error_linear)
mse_linear <- 1/n_total * sse_linear
r2_linear <- (sse_null - sse_linear)/sse_null
bic_linear <- n_total * log(mse_linear) + 2 * log(n_total)
```

- We can compare the models using a table:

| Model | Parameters | MSE | $R^2$ | BIC |
|-------|:----------:|:---------------------:|:-----:|:--------------------:|
| Null  | 1          | `r round(mse_null,0)` | NA    | `r round(bic_null,0)`|
| Classes Missed  | 2          | `r round(mse_linear,0)` | `r round(r2_linear,2)` | `r round(bic_linear,0)`|

--

- This means that including the classes missed by a student as a predictor of
final grade improves the model in comparison to using the mean.

---

## Interpretation

- The value of $R^2$ is interpreted the same way as before, it is the proportion
of error accounted for by the model in comparison to the Null model.

--

- Now we can interpret the values of the coefficients $\hat{\beta}_0$ and 
$\hat{\beta}_1$.

--

  - Students that missed 0 classes have on average a final grade of approximately
  `r round(betas[1],0)` $(\hat{\beta}_0)$.
  
--

  - According to the model, the number of points that a student is expected to 
  lose for every class missed is approximately `r round(betas[2],0)` 
  $(\hat{\beta}_1)$.
  
--

- Using `ggplot` we can visualize the predictions of a simple linear regression.

--

- We call this type of data visualization a `scatterplot`.

---

```{r scatter-code, include = FALSE}
ggplot(data = grades) + #BREAK
  aes(y = grade) + #BREAK
  aes(x = classes_missed) + #BREAK
  geom_point(color = "#0D95D0", alpha = 0.5, size = 3) + #BREAK
  xlab("Classes Missed") + #BREAK
  ylab("Final Grade") + #BREAK
  theme_classic() + #BREAK
  theme(axis.title.x = element_text(size = 20),
        axis.title.y = element_text(size = 20)) +
  geom_smooth(method = lm, se = FALSE, color = "#774fa0")
```

`r chunk_reveal(chunk_name = "scatter-code", title = "### Scater plot grade by classes missed")`

---

## Correlation and Simple linear regression

- Remember that the correlation is a measure of the association between two 
dependent variables.

--

- The visualization of the simple linear regression looks very similar to the 
correlation between two variables.

--

- This is because the correlation and the slope of a line are two concepts
that are related, but are not the same thing.

--

- For example, if the correlation between classes missed and grades was 0 
(R = 0), then if we use one of those variables as a predictor (in a simple 
linear regression $x_i$) the slope of that regression line $(\beta_1)$ would 
also be equal to 0.

--

- If the slope of a simple linear regression is equal to 0 $(\beta_1 = 0)$, then 
the correlation (R) between the two variables would also be equal to 0.

--

- However, correlation and **slope** are two different concepts, 
and they will not be equal in the majority of the cases.

---

## Example: Same correlation, different slope

.pull-left[
```{r constant-cor, echo=FALSE,fig.align='center'}
x <- MASS::mvrnorm(200,c(0,0), rbind(c(1,0.3),c(0.3,1)))
x1 <- cbind(x[,1], x[,2]) 
plot(x1[,2], x1[,1], axes = F, ann = F, pch = 16,
     col = "#0D95D0", cex = 1.3, xlim = c(-3,3), ylim = c(-3,3))
box(bty = "l")
axis(side = 1)
axis(side = 2, las = 2)
mtext(text = "X", side = 1, line = 2.6, cex = 1.8)
mtext(text = "Y", side = 2, line = 2.6, cex = 1.8)
mtext(text = bquote(paste("R = ", .(round(cor(x1[,1], x1[,2]), 3)))),
      cex = 2, side = 3, at = -1.5)
mtext(text = bquote(paste(hat(beta)[1], " = ", .(round(lm(x1[,1]~x1[,2])$coef[2],3)))),
      cex = 2, side = 3, at = 1.5)
abline(a = lm(x1[,1]~x1[,2])$coef[1], b = lm(x1[,1]~x1[,2])$coef[2], lwd = 3, 
       col = "#774fa0")
```
]

.pull-right[
```{r constnat-cor2,echo=FALSE,fig.align='center'}
x2 <- cbind(0.5 * x[,1], x[,2]) 
plot(x2[,2], x2[,1], axes = F, ann = F, pch = 16,
     col = "#0D95D0", cex = 1.3, xlim = c(-3,3), ylim = c(-3,3))
box(bty = "l")
axis(side = 1)
axis(side = 2, las = 2)
mtext(text = "X", side = 1, line = 2.6, cex = 1.8)
mtext(text = "Y", side = 2, line = 2.6, cex = 1.8)
mtext(text = bquote(paste("R = ", .(round(cor(x2[,1], x2[,2]),3)))),
      cex = 2, side = 3, at = -1.5)
mtext(text = bquote(paste(hat(beta)[1], " = ", .(round(lm(x2[,1]~x2[,2])$coef[2],3)))),
      cex = 2, side = 3, at = 1.5)
abline(a = lm(x2[,1]~x2[,2])$coef[1], b = lm(x2[,1]~x2[,2])$coef[2], lwd = 3, 
       col = "#774fa0")
```
]

---

## Example: Different correlation, same slope

.pull-left[
```{r constant-b, echo=FALSE,fig.align='center'}
x <- rnorm(n = 200, mean = 0, sd = 1)
ep <- rnorm(200, 0, 1)
x1 <- cbind(0.5 * x + ep,x) 
plot(x1[,2], x1[,1], axes = F, ann = F, pch = 16,
     col = "#0D95D0", cex = 1.3, xlim = c(-3,3), ylim = c(-3,3))
box(bty = "l")
axis(side = 1)
axis(side = 2, las = 2)
mtext(text = "X", side = 1, line = 2.6, cex = 1.8)
mtext(text = "Y", side = 2, line = 2.6, cex = 1.8)
mtext(text = bquote(paste("R = ", .(round(cor(x1[,1], x1[,2]), 3)))),
      cex = 2, side = 3, at = -1.5)
mtext(text = bquote(paste(hat(beta)[1], " = ", .(round(lm(x1[,1]~x1[,2])$coef[2],3)))),
      cex = 2, side = 3, at = 1.5)
abline(a = lm(x1[,1]~x1[,2])$coef[1], b = lm(x1[,1]~x1[,2])$coef[2], lwd = 3, 
       col = "#774fa0")
```
]

.pull-right[
```{r constnat-b2,echo=FALSE,fig.align='center'}
x2 <- cbind(0.5 * x + 0.5 * ep,x)
plot(x2[,2], x2[,1], axes = F, ann = F, pch = 16,
     col = "#0D95D0", cex = 1.3, xlim = c(-3,3), ylim = c(-3,3))
box(bty = "l")
axis(side = 1)
axis(side = 2, las = 2)
mtext(text = "X", side = 1, line = 2.6, cex = 1.8)
mtext(text = "Y", side = 2, line = 2.6, cex = 1.8)
mtext(text = bquote(paste("R = ", .(round(cor(x2[,1], x2[,2]),3)))),
      cex = 2, side = 3, at = -1.5)
mtext(text = bquote(paste(hat(beta)[1], " = ", .(round(lm(x2[,1]~x2[,2])$coef[2],3)))),
      cex = 2, side = 3, at = 1.5)
abline(a = lm(x2[,1]~x2[,2])$coef[1], b = lm(x2[,1]~x2[,2])$coef[2], lwd = 3, 
       col = "#774fa0")
```
]

---

## Correlation and slope

- The correlation and the slope in simple linear regression express similar ideas.

--

- The correlation is the association between two variables and therefore does
not depend on the units of measurement (for example, weight in Kilograms vs
weight in pounds).

--

- On the other hand, the slope in a simple linear regression measures the 
impact or change in a dependent variable for a **unit** change in our 
independent variable, and therefore it will depend on the unit of measure.
