---
title: "Lecture 7"
subtitle: "Paired Samples"
author: "Psych 10 C"
institute: "University of California, Irvine"
date: "04/13/2022"
output:
  xaringan::moon_reader:
    self_contained: true
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-extra, echo = FALSE}
xaringanExtra::use_tile_view()

xaringanExtra::use_fit_screen()

xaringanExtra::use_panelset()

xaringanExtra::use_editable(expires = 1)

xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)

htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```

```{r load-tidy, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)

knitr::opts_chunk$set(fig.width = 6, message = FALSE, 
                      warning = FALSE, comment = "", 
                      cache = F)
link <- "https://raw.githubusercontent.com/ManuelVU/psych-10c-data/main/exams-example.csv"
exams <- read_csv(file = link)
```

## Paired samples

- Until now we have been working with an experimental design known as
**between subjects** design.

--

- This means that each participant in our sample can only be included in one 
group at a time.

--

- For example, in the smokers data, participant can be either on the smokers 
group or on the non-smokers group but they can't be in both.

--

- We call this a **between subjects** design because we want to compare two 
groups that are made out of the responses of different participants.

--

- However, we could have a design where we measure a participant in both groups. 

---

## Example

- **Problem:** We are at the end of the semester at a university and we want to
know if our students have improved their grades. 

--

- There is variability in grades but we would like to make a generalization, and
say that either they improved during the semester or that they didn't 
(as a group).

--

- To solve this problem we have access to the students' scores on the midterm 
and final exams.

--

- Notice that the observations between the groups will be correlated. 
This is a problem for our approach, as we stated in our model that the 
observations that we had where independent!

--

- The correlation in our observations comes from the fact that we are measuring
the same participant twice. Student's 1 midterm and final score will have some
degree of correlation because they come from the same person.

--

- In order to avoid the problem of correlated observations in this sort of 
context (observations are correlated because they come from the same participant)
we need to get rid of the "redundant" information.

---

## Example

- A simple way to get rid of the correlation is by just taking the difference 
between the two scores (this will only work when the correlation comes from 
us having two observations of the same participant).

--

- In this case by taking the difference between midterm and final scores we are 
trying to remove some amount of redundant information. 

--

- This also means that we have to make some changes to our models. 

---

class: inverse, center, middle

# Null model

---

## Null model

- To define our models we now need to add one step, the difference between 
our observations. Remember that this time we are interested in a model about 
the differences between scores.

--

- We denote the *i-th* observation of the midterm score as $y_{i1}$ and the 
first observation of our final score as $y_{i2}$, then we define the difference 
between final and midterm as: $$d_{i} = y_{i2} - y_{i1}$$

--

- Notice that now we only have one indicator for the difference, in other words, 
we only have a difference in score for each participant (there are no groups).

--

- The null model assumes that there are no differences between midterm and 
final scores: $$d_i \sim \text{Normal}(0,\sigma_0^2)$$

--

- Notice that we still assume that there will be variability in our 
observations $(\sigma_0^2 > 0)$. However, we expect the difference between 
final and midterm scores to be around $0$.

---

class: inverse, center, middle

# Effects model

---

## Effects model

- Using the same notation, the Effects model will formalize our assumption that
there will be a difference between final and midterm scores. We express this 
model as: $$d_i \sim \text{Normal}(\mu,\sigma_e^2)$$

--

- This means that we expect our observations to follow a normal distribution
centered around some value $\mu$. With a variance of $\sigma_e^2$.

--

- As in the previous examples, we have two models that we want to use in order 
to answer our research question.

--

- Do students scores improve between the midterm and final exams?

---

class: inverse, center, middle

# Data analysis

---

## Variables and data visualization

.panelset[
  .panel[.panel-name[Visualization]
- Let's start by visualizing our data using a histogram:
  ]
  .panel[.panel-name[Code]
```{r hist-score, eval = FALSE}
ggplot(data = exams) + 
  aes(x = scores) + 
  aes(fill = exam, color = exam) + 
  geom_histogram(position="identity", 
                 binwidth = 2,  
                 alpha = 0.3) + 
  theme_classic() + 
  xlab("Exam Scores") + 
  ylab("Frequency") + 
  guides(fill = guide_legend("Exam"), color = "none") + 
  theme(axis.title.x = element_text(size = 20),
        axis.title.y = element_text(size = 20))  
```
  ]
  .panel[.panel-name[Plot]
```{r hist-score-out, ref.label = "hist-score", fig.align = 'center', echo = FALSE, fig.width = 8, fig.height = 6.5}

```
  ]
]

---

## Removing dependency between observations

.panelset[
  .panel[.panel-name[New variable]
- Now create a new variable (part 2 of the "exams-example.Rmd" file) using the 
difference between final scores and midterm.

- Make a box plot using the new difference variable.
  ]
  .panel[.panel-name[Code]
```{r exams-wide, echo = FALSE, message = FALSE}
exams <- exams %>% 
  pivot_wider(names_from = exam, 
              values_from = scores)
```

```{r bp-diff, eval = FALSE}
# Creating a difference variable
exams <- exams %>% 
  mutate("difference" = final - midterm)
# Box plot
ggplot(data = exams) + 
  aes(y = difference) + 
  theme_classic()+
  geom_boxplot(fill = "white", col = "dodgerblue4") + 
  ylab("Difference in scores") + 
  guides(fill = "none", color = "none") + 
  theme(axis.title.x = element_text(size = 20),
        axis.title.y = element_text(size = 20))
```
  ]
  .panel[.panel-name[Plot]
```{r bp-diff-out, ref.label = "bp-diff", fig.align = 'center', echo = FALSE, fig.width = 5.5, fig.height = 6}

```
  ]
]

---

## Models' predictions, SSE and mean SE.

.panelset[
  .panel[.panel-name[Predictions and error]
- Add the predictions of the Null and Effects model to the exams data 
(question 4).

- Using the predictions of each model add a new variable with the squared error
of each observation (question 5).

- Calculate the SSE and mean Squared Error using the last two variables you 
added (question 6).
  ]
  .panel[.panel-name[Code]
```{r pred-sse-mse}
# number of participants
n_total <- nrow(exams)
# add predictions
exams <- exams %>% 
  mutate("null_pred" = rep(x = 0, times = n_total),
         "eff_pred" = rep(x = mean(difference), times = n_total))
# add squared errors
exams <- exams %>% 
  mutate("null_error" = (difference - null_pred)^2,
         "eff_error" = (difference - eff_pred)^2)
# compute SSE and mSE by model
errors <- exams %>% 
  summarise("sse_0" = sum(null_error),
            "sse_e" = sum(eff_error),
            "mse_0" = 1/n_total * sum(null_error),
            "mse_e" = 1/n_total * sum(eff_error))
```
  ]
  .panel[.panel-name[Null]
- Prediction: $0$.

- SSE: `r round(errors$sse_0,2)`

- m-SE: `r round(errors$mse_0,2)`
  ]
  .panel[.panel-name[Effects]
- Prediction: `r round(mean(exams$difference),2)`.

- SSE: `r round(errors$sse_e,2)`

- m-SE: `r round(errors$mse_e,2)`
  ]
]

---

## Model Evaluation

.panelset[
  .panel[.panel-name[$R^2$ and BIC]
- Now we want to evaluate our models, we have to start by calculating the 
proportion of error (variance) accounted for by the Effects model $(R^2)$ 
(question 7).

- What proportion of the error (variance) is accounted by the Effects model? 
(question 8)

- What are the BIC values of the Null and Effects models? (question 9)
  ]
  .panel[.panel-name[Code]
```{r r2-bic}
# proportion of error accounted for 
r_2 <- (errors$sse_0 - errors$sse_e) / errors$sse_0
# BIC by model
bic_model <- c(n_total * log(errors$mse_0),
               n_total * log(errors$mse_e) + log(n_total))
```
  ]
  .panel[.panel-name[$R^2$]
  
- The proportion of the error accounted for by the Effects model was `r round(r_2,2)`.

- The BIC of the Null model was `r round(bic_model[1],3)`.

- The BIC of the Effects model was `r round(bic_model[2],3)`.
  ]
  .panel[.panel-name[Conclusion]

- According to the BIC of the models, the effects model is the one that can 
account for the difference in students' midterm and final scores. 

- Students received a score that was `r round(mean(exams$difference),2)` points 
higher on average for the final exam in comparison to the midterm.
  ]
]