---
title: "Lecture 17"
subtitle: "Correlation and Linear Regression"
author: "Psych 10 C"
institute: "University of California, Irvine"
date: "05/09/2022"
output:
  xaringan::moon_reader:
    self_contained: true
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-extra, echo = FALSE}
xaringanExtra::use_tile_view()

xaringanExtra::use_fit_screen()

xaringanExtra::use_editable(expires = 1)

xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)

htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```

```{r load-tidy, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)

knitr::opts_chunk$set(fig.width = 6, message = FALSE, 
                      warning = FALSE, comment = "", 
                      cache = F)
```

## Correlation

- Up to this point we have looked at problems with a single dependent variable, 
however, we could have more than one.

--

- When we have 2 dependent variables we could ask ourselves, how are those two 
variables related?

--

- The correlation between two variables is a measure of the association between 
them.

--

- Formally, it's a measure of how the two variables change together. For 
example, what happens to the values of one variable as the second one increases?

--

- The correlation coefficient measures this degree of association and we denote
this value with $R$.

---

## Correlation

- The correlation coefficient can take positive or negative values or it can 
be 0.

--

- However, it will always be bounded between -1 and 1, for example:

.pull-left[
```{r cor-1,echo=FALSE, fig.align='center'}
iq <- rnorm(50, 100, 10)
a <- 15 - 3/20 * 100
b <- 3/20
plot(iq, a + b * iq, axes = F, ann = F, pch = 16,
     col = "#0D95D0", cex = 1.3)
box(bty = "l")
axis(side = 1)
axis(side = 2, las = 2)
mtext(text = "IQ", side = 1, line = 2.6, cex = 1.8)
mtext(text = "Depression", side = 2, line = 2.6, cex = 1.8)
mtext(text = "R = 1", cex = 2, side = 3)
```
]
.pull-right[
```{r cor-2,echo=FALSE, fig.align='center'}
a <- 15 + 3/20 * 100
b <- 3/20
plot(iq, a - b * iq, axes = F, ann = F, pch = 16,
     col = "#0D95D0", cex = 1.3)
box(bty = "l")
axis(side = 1)
axis(side = 2, las = 2)
mtext(text = "IQ", side = 1, line = 2.6, cex = 1.8)
mtext(text = "Depression", side = 2, line = 2.6, cex = 1.8)
mtext(text = "R = -1", cex = 2, side = 3)
```
]

---

## Correlation

- A correlation of 0 should look like this:

```{r cor-0, echo=FALSE,fig.align='center', fig.width=7}
x <- MASS::mvrnorm(200,c(15,100), c(1,10) * diag(2))
plot(x[,2], x[,1], axes = F, ann = F, pch = 16,
     col = "#0D95D0", cex = 1.3)
box(bty = "l")
axis(side = 1)
axis(side = 2, las = 2)
mtext(text = "IQ", side = 1, line = 2.6, cex = 1.8)
mtext(text = "Depression", side = 2, line = 2.6, cex = 1.8)
mtext(text = "R = 0", cex = 2, side = 3)
```

---

## Correlation

- Therefore, the sign of the correlation indicates whether one variable 
**increases** as the other one **increases** (**positive R**) or if one variable 
**decreases** as the other one **increases** (**negative R**).

--

- On the other hand, the magnitude of R indicates the "strength" of the 
association, with values closer to 1 (or -1) representing a stronger association
between the variables.

--

.pull-left[
```{r cor-08, echo=FALSE,fig.align='center', fig.width=7}
x <- MASS::mvrnorm(n = 200, mu = c(0,0), Sigma = rbind(c(1,0.8),c(0.8,1)))
x[,1] <- 15 + x[,1]
x[,2] <- 100 + sqrt(10)*x[,2]
plot(x[,2], x[,1], axes = F, ann = F, pch = 16,
     col = "#0D95D0", cex = 1.3,ylim=c(11,19))
box(bty = "l")
axis(side = 1)
axis(side = 2, las = 2)
mtext(text = "IQ", side = 1, line = 2.6, cex = 1.8)
mtext(text = "Depression", side = 2, line = 2.6, cex = 1.8)
mtext(text = "R = 0.8", cex = 2, side = 3)
```

]
.pull-right[
```{r cor-02, echo=FALSE,fig.align='center', fig.width=7}
x <- MASS::mvrnorm(200,c(0,0), rbind(c(1,0.3),c(0.3,1)))
x[,1] <- 15 + x[,1]
x[,2] <- 100 + sqrt(10)*x[,2]
plot(x[,2], x[,1], axes = F, ann = F, pch = 16,
     col = "#0D95D0", cex = 1.3, ylim=c(11,19))
box(bty = "l")
axis(side = 1)
axis(side = 2, las = 2)
mtext(text = "IQ", side = 1, line = 2.6, cex = 1.8)
mtext(text = "Depression", side = 2, line = 2.6, cex = 1.8)
mtext(text = "R = 0.3", cex = 2, side = 3)
```
]

---

## Correlation

- An example of a negative correlation would look like:

.pull-left[
```{r cor-n08, echo=FALSE,fig.align='center', fig.width=7}
x <- MASS::mvrnorm(n = 200, mu = c(0,0), Sigma = rbind(c(1,-0.8),c(-0.8,1)))
x[,1] <- 15 + x[,1]
x[,2] <- 100 + sqrt(10)*x[,2]
plot(x[,2], x[,1], axes = F, ann = F, pch = 16,
     col = "#0D95D0", cex = 1.3,ylim=c(11,19))
box(bty = "l")
axis(side = 1)
axis(side = 2, las = 2)
mtext(text = "IQ", side = 1, line = 2.6, cex = 1.8)
mtext(text = "Depression", side = 2, line = 2.6, cex = 1.8)
mtext(text = "R = - 0.8", cex = 2, side = 3)
```

]
.pull-right[
```{r cor-n02, echo=FALSE,fig.align='center', fig.width=7}
x <- MASS::mvrnorm(200,c(0,0), rbind(c(1,-0.3),c(-0.3,1)))
x[,1] <- 15 + x[,1]
x[,2] <- 100 + sqrt(10)*x[,2]
plot(x[,2], x[,1], axes = F, ann = F, pch = 16,
     col = "#0D95D0", cex = 1.3, ylim=c(11,19))
box(bty = "l")
axis(side = 1)
axis(side = 2, las = 2)
mtext(text = "IQ", side = 1, line = 2.6, cex = 1.8)
mtext(text = "Depression", side = 2, line = 2.6, cex = 1.8)
mtext(text = "R = - 0.3", cex = 2, side = 3)
```
]

---

## Correlation

- Regardless of the sign of a correlation, we say that a correlation is stronger
the closer it is to 1 (or -1).

--

- For example, a correlation of 0.5 implies a stronger association between the 
variables than a correlation 0f 0.2.

--

- However, a correlation of $-0.3$ also implies a stronger association between the 
variables than a correlation of $0.2$

--

- Correlation measures the association between two variables, however, in this 
class we are not interested on association but on using one variable to 
**predict** the values of another.

---

## Linear Regression

- To this point we have been using categorical variables in order to make 
predictions about data. However, what should we do if we have a continuous
independent variable, or a variable that can take many different values like 
income?

--

- For example, say that we want to predict a student's grade based on the number
of classes that they missed during a quarter?

--

What is the independent variable in this example?

.can-edit.key-likes[
  - **ANS:**
]

--

What is the dependent variable in this example?

.can-edit.key-likes[
  - **ANS:**
]

--

- How can we make use of the information from our independent variable to 
predict the values of our dependent variable?

---

## Linear Regression

- Let's look at some data first, we have the number of classes missed by 4 
students and their corresponding grades on a statistics class:

--

.pull-left[
```{r miss-grade, echo=FALSE, fig.align='center',fig.width=7, fig.height=7}
x <- c(2,4,6,8)
y <- 98 - 5.2 * x + c(7,-13,11,-5)
plot(x, y, axes = F, ann = F, pch = 16,
     col = "#0D95D0", cex = 2, ylim=c(50,100))
box(bty = "l")
axis(side = 1)
axis(side = 2, las = 2)
mtext(text = "Missed classes", side = 1, line = 2.6, cex = 1.8)
mtext(text = "Grade", side = 2, line = 2.6, cex = 1.8)
```
]
.pull-right[
```{r miss-grade-lm, echo=FALSE, fig.align='center',fig.width=7, fig.height=7}
x <- c(2,4,6,8)
y <- 98 - 5.2 * x + c(7,-13,11,-5)
plot(x, y, axes = F, ann = F, pch = 16,
     col = "#0D95D0", cex = 2, ylim=c(50,100))
box(bty = "l")
axis(side = 1)
axis(side = 2, las = 2)
mtext(text = "Missed classes", side = 1, line = 2.6, cex = 1.8)
mtext(text = "Grade", side = 2, line = 2.6, cex = 1.8)
curve(98 - 5.2 * x, from = 2, to = 8, lwd = 2.5, col = "#774fa0", add = TRUE)
```
]

---

## Linear Reression

- To this point we have used averages to predict the values of our observations
in an experiment (averages by group or by combinations of factor levels). The 
mean was our best prediction for the expected value of a dependent variable.

--

- However, when we have a continuous independent variable or a numeric variable 
that can take many values, we can use lines instead of averages.

--

- In other words, we will use the equation of a line to predict the values of 
our dependent variable using the value of our independent variable.

--

- The equation for the line is: $$\beta_0 + \beta_1x_i$$

--

- Where $x_i$ represents the value of our independent variable associated with 
our $i-th$ observation.

--

- In our previous example, $x_1 = 2$ which means that the first student in the 
sample missed two classes, and $x_2 = 8$ which means that the second student 
in our sample missed 8 classes.

---

## Parameters of the equation line

- There are 2 parameters in the equation line from the previous slide:

--

  1. $\beta_0$: which is known as  **the intercept** and it represents our 
  prediction when the independent variable is set to 0 (prediction when $x = 0$).
  
--

  2. $\beta_1$: which is known as **the slope** and it represents how much our
  dependent variable changes when the value of our independent variable $x$ 
  increases by **one** unit.
  
--

- In our grades and missed classes example, **the intercept** $\beta_0$ would be
interpreted as the predicted grade of a student that didn't miss a single class.

--

- On the other hand, **the slope** $\beta_1$ would be interpreted as the number 
of points that a student is predicted to loose (because the line is going down)
for each missed class.

---

### How do we actually estimate the parameters of a model?

- When we started comparing two groups we said that the best prediction (or the 
estimator) for the expected value $(\mu)$ was the average of the responses.
Because of that we used the average of our dependent variable as a prediction.

--

- Well that's not how it actually works...

--

- If you remember, the only step that has remained constant for the methods
that we have learned in this class is that we always calculate the **Error** of
the model the same way. 

--

- To evaluate every model we always need to obtain the Sum of Squared Errors
of said model. 